<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Final Report</title>
    <link rel="icon" type="image/x-icon" href="favicon.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <div class="topnav" id = "topNavElem">
        <a href="https://shayan-boghani.github.io/cs4641project">CS 4641 Group 3 Project</a>
        <div class="topnav-right">
            <a href="https://shayan-boghani.github.io/cs4641project/proposal">Proposal</a>
            <a href="https://shayan-boghani.github.io/cs4641project/midterm-report">Midterm Report</a>
            <a class = "active" href="https://shayan-boghani.github.io/cs4641project/final-report">Final Report</a>
            <a href="javascript:void(0);" class="icon" onclick="myFunction()">
                <i class="fa fa-bars"></i>
            </a>
        </div>
      </div>
    <h1>Final Report</h1>
    <img src="Updated Infographic.png" alt="Infographic" class="center">
    <h2>Introduction</h2>
        <p>The music streaming industry is one of the largest sectors of the entertainment industry, representing a 10.2 billion dollar revenue stream in 2022 alone [1]. This industry is also highly competitive, and players are consistently looking at methods to optimize the listening experience for their users. One of the primary methods in which this is accomplished is through song recommendations based on listening patterns. Currently, this is primarily done using concrete attributes of music, such as tempo, lyrics, and artists to name a few. While this is an adequate first step, there are methods in which this can be improved. Emotion and music are significantly interlinked, with emotions affecting music choice and vice versa [2]. Singh & Goyal (2018) discuss the effect music has on emotion, and this is the motivation behind focusing on this new classification model [3]. Our team aims to improve the song recommendation concept by developing a tool that can classify songs on Spotify based on the primary emotion presented by the song. The primary emotions will be classified using Thayer’s mood model [4]. By developing this additional classification, our group sees potential in the application of this technology to further improve music recommendation technology and provide a better overall user experience.</p>
    <h2>Methodology</h2>
        <p>The dataset being used is the “278k Emotion Labeled Spotify Songs” found on Kaggle [6]. The dataset contains 11 features describing 278,000 songs. The features include the metrics Acousticness, Danceability, Energy, Instrumentalness, Liveliness, Speechiness, Valence, Loudness, and Tempo. All features are continuous numeric values, and all except Loudness, Tempo, Duration, and Spec_Rate features are limited to a range of 0 to 1. The target values for each song are discrete integers from 0 to 3, with each integer representing a different mood. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm. </p>
        <p>The first steps involve preprocessing and standardizing the dataset. Most of the features are already normalized, so the four non-normalized features (Loudness, Tempo, Duration, and Spec_Rate) will be normalized to the same [0, 1] scale. Since all features are already represented numerically, there is no need for additional feature engineering. Next, we will be looking at the variances, covariances, and normalized mutual information with the target values to better understand the relationship between the features and to determine which features can be dropped. With the remaining features selected, we plan to utilize Linear PCA to reduce our reduced feature space to a smaller number of principle components that can represent at least 80% of the variance in the data [5].</p>
        <p>The methods utilized for unsupervised learning were K-Means, GMM, and DBScan each utilized for clustering. These methods will focus on identifying patterns and groups based on the reduced feature space from our dimensionality reduction. The performance of each clustering method will be evaluated with the Folkes-Mallows measure, Normalized Mutual Information, Adjusted Rand statistic, Davies-Bouldin index, Jaccard coefficient, and Recall score.</p>
        <p>Additionally, two supervised learning techniques were selected: Neural Network and Random Forest. Neural networks are known for being robust, flexible, and applicable for both classification and regression problems, so using a neural network felt like a logical choice. The random forest was chosen to determine the effectiveness of an ensemble learning technique for our problem . Accuracy, Precision, and Recall were used to determine the effectiveness of the supervised learning techniques.</p>
    <h2>Results and Discussion</h2>
    <h3>Feature Selection and Dimensionality Reduction</h3>
    <p>After all features were normalized to the same [0, 1] scale, statistical feature selection was conducted. First, the variances and correlations of all features were calculated and are shown in the figures below.</p>
    <img src="Final Report Pictures/Final-Figure1.png" alt="Figure 1" class="center">
    <figcaption>Figure 1: Variances of all features in the dataset.</figcaption>
    <img src="Final Report Pictures/Final-Figure2.png" alt="Figure 2" class="center">
    <figcaption>Figure 2: Correlation Matrix for all features in the dataset.</figcaption>
    <p>The variance chart in Figure 1 shows that some features have high variances, indicating features such as instrumentalness and acousticness could be helpful to classify songs. Both duration and sample rate of the songs had incredibly low variance values, so those two features were dropped. Logically, it also follows that the sample rate and length of the song will likely not affect the mood of the song. The correlation matrix in Figure 2 shows the relationships between each feature, and the following pairs of variables have a high correlation (higher than ± 0.6): Loudness and Energy, Acousticness and Energy, Acousticness and Loudness, and Speechiness and Spec Rate. We can ignore the final pairing since Spec Rate is already being dropped. Of the remaining pairings, one of the features can be dropped to reduce the feature space. However, since all of these variables (Energy, Acousticness, and Loudness) share high mutual information with the target (Figure 3), none of these variables were dropped in the end.</p>
    <img src="Final Report Pictures/Final-Figure3.png" alt="Figure 3" class="center">
    <figcaption>Figure 3: Mutual information graph between all original eleven features and the target.</figcaption>
    <p>The chart in Figure 3 shows shared mutual information values with the target, and the liveness and tempo features were also ultimately dropped; of all remaining features, those two had the lowest mutual information with the target.</p>
    <p>Looking at these statistics allowed for the feature space to be simplified to 7 components: Acousticness, Loudness, Energy, Instrumentalness, Danceability, Valence, and Speechiness. From here, Linear PCA was selected to reduce the complexity of our problem further; the initial dataset distribution also implied a linear separation of data, making Linear PCA a good fit. The results of the PCA can be seen in Figure 5.</p>
    <img src="Final Report Pictures/Final-Figure4.png" alt="Figure 4" class="center">
    <figcaption>Figure 4: Scree plot for the Spotify Song Dataset</figcaption>
    <p>The goal was to reduce the problem to a number of principle components until the total recovered variance is at least above 80%. With the PCA on the simplified feature space, we found that the use of 3 principal components allowed us to represent the feature space with 88% of the variance recovered, exceeding our thresholds. As a result of all the feature selection and dimensionality reduction steps, the problem was reduced from an 11-dimensional feature space to a 3-dimensional feature space with three principal components.</p>
    <h3>Unsupervised Learning Results and Discussion</h3>
    <img src="Final Report Pictures/Final-Figure5.png" alt="Figure 5" class="center">
    <figcaption>Figure 5: Distribution of songs in the dataset. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm.</figcaption>
    <p>Before clustering, it is important to first understand the distribution of the four classes in our dataset. Figure 5 shows the frequency of each target class in the dataset, with the number of happy songs (mood 1) being much larger than the other classes, exceeding 100,000 songs out of 278,000 total. Thus, we can expect the cluster of happy songs to contain the most data points. While clustering will not be affected by the class imbalance, the data must be rebalanced prior to supervised learning techniques.</p>
    <h4>K-Means</h4>
    <p>After reducing dimensionality, K-Means was used to identify any obvious clusters. With each of the three principal components mapped against each other, as seen in the graphs, there are no obvious clusters. This can be checked with the Fowlkes-Mallows, Rand, and NMI scores. None of the clusterings resulted in a Fowlkes-Mallows score above 0.55 while none had a Rand or NMI score above 0.38. This further justifies the poor clustering with K-Means. Two of the clusterings, however, had a correct classification just below fifty percent. This is improved upon a mere random guess, which would have a correct classification rate of about twenty-five percent.</p>
    <p>The relationship between all three principal components can also be modeled with K-Means. Similar to the two-dimensional relationships, the model gave very little insight into effective clustering of the data points. As can be noted in Figure 9, it is very difficult to gain any information about the classes from the model.</p>
    <p>After looking at the results, it was clear the K-Means could not effectively cluster the data so GMM and DBSCAN should also be computed to compare the different clustering methods.</p>
    <img src="Final Report Pictures/Final-Figure6.png" alt="Figure 6" class="center">
    <figcaption>Figure 6: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure7.png" alt="Figure 7" class="center">
    <figcaption>Figure 7: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure8.png" alt="Figure 8" class="center">
    <figcaption>Figure 8: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure9.png" alt="Figure 9" class="center">
    <figcaption>Figure 9: Graphs representing all three principal components. Original clusters graph on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <h4>GMM</h4>
    <p>Along with K-Means, GMM was utilized for clustering. The GMM model was run for each of the three principal components compared to one another (1-2, 1-3, 2-3). The results showed that GMM was not a strong clustering technique when used with this data set. This is quantified using the Fowlkes-Mallows, Rand, and NMI scores. The maximum Fowlkes-Mallows score recorded was 0.491 and the maximum scores for Rand and NMI were 0.3 and 0.32, respectively. All these values were recorded for the comparison of principal components 1 and 3. The maximum clustering correctness was recorded at approximately sixty-five percent. This value is significantly higher than the randomized outcome which is expected which is approximately twenty-five percent.</p>
    <p>Something to note is that GMM is not suited for categorical features, in this case the groupings of songs based on emotions. Given the assumption of normal distribution of all features, clustering can be quite difficult. As seen from the results, this is the case with this data set. It can be concluded that GMM is not a valid approach for this data set based on the fundamental issues associated with utilizing the method for categorical data.</p>
    <img src="Final Report Pictures/Final-Figure10.png" alt="Figure 10" class="center">
    <figcaption>Figure 10: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure11.png" alt="Figure 11" class="center">
    <figcaption>Figure 11: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure12.png" alt="Figure 12" class="center">
    <figcaption>Figure 12: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure13.png" alt="Figure 13" class="center">
    <figcaption>Figure 13: Graphs representing Principal Component 1 (X-axis), Principal Component 2 (Y-axis), and Principal Component 3 (Z-axis). Original clusters graph is on the left. Predicted clusters graph using GMM on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <h4>DBScan</h4>
    <p>In addition to K-Means and GMM, DBScan was used in clustering. As with both other clustering methods, DBScan was run with 3 combinations of principal components – 1-2, 1-3 and 2-3. To optimize the clustering methodology, MinPts was found to be 4 due to the use of 3 principal components, and eps was found to be 0.003 using the elbow method (Figure 14).</p>
    <img src="Final Report Pictures/Final-Figure14.png" alt="Figure 14" class="center">
    <figcaption>Figure 14: Eps calculation based on 4th nearest neighbor. Eps used: 0.003</figcaption>
    <p>This clustering method was found to be rather poor in representing the clusters of this dataset. This was numerically justified through the use of the Fowlkes-Mallows, Rand, and NMI clustering evaluation scores. The maximum scores recorded for these metrics were 0.512, 0.187, and 0.217 respectively. In addition, the maximum accuracy using this method was only around 27%, just slightly higher than random selection which is 25%. The clusters of each combination of principal components, as well as all principal components together can be visualized in Figures 15-18 below.</p>
    <p>Multiple reasons for this inaccuracy are possible. One of the major limitations of DBScan is that it is not optimal at clustering datasets with varying densities, which is a possibility in this case. In addition, DBScan is highly dependent on the value of the eps parameter. While visually optimized using the elbow method and the above graph, it is possible that the true optimal parameter was not found.</p>
    <img src="Final Report Pictures/Final-Figure15.png" alt="Figure 15" class="center">
    <figcaption>Figure 15: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure16.png" alt="Figure 16" class="center">
    <figcaption>Figure 16: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure17.png" alt="Figure 17" class="center">
    <figcaption>Figure 17: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <img src="Final Report Pictures/Final-Figure18.png" alt="Figure 18" class="center">
    <figcaption>Figure 18: Graphs representing Principal Component 1 (X-axis), Principal Component 2 (Y-axis), and Principal Component 3 (Z-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</figcaption>
    <h4>Unsupervised Learning Methods Comparison</h4>
    <img src="Final Report Pictures/Final-Figure19.png" alt="Figure 19" class="center">
    <figcaption>Figure 19: Graphs comparing the scores for Fowlkes-Mallows, NMI and Adjusted Rand of each clustering technique: KMeans, DBScan, and GMM respectively.</figcaption>
    <img src="Final Report Pictures/Final-Figure20.png" alt="Figure 20" class="center">
    <figcaption>Figure 20: Graphs comparing the scores for Davies-Bouldin, Jaccard and Recall of each clustering technique: KMeans, DBScan, and GMM respectively.</figcaption>
    <p>The clustering techniques performed most similarly when compared using the Fowlkes Mallows score. However, though DBScan performed better than both GMM and KMeans when scored by Fowlkes Mallows, it yielded the least accuracy of proper classification. The better scoring measures for this application are NMI and Adjusted Rand.  These more accurately reflect the effectiveness of the predictiveness of the clustering techniques in relation to each other. Of the three clustering techniques, KMeans had the best accuracy at nearly 49%, with GMM trailings at about 40%. This relationship is best reflected by the NMI score. While not the best indicator for this application, the Davies Bouldin, Jaccard, and Recall scores were also calculated for each technique for comparison. GMM scored highly against the Jaccard score and Recall score, indicating that this technique led to the best clusters, yet it did not provide the best prediction results.</p>
    <p>Overall, KMeans clustering was the best unsupervised learning clustering technique, correctly labeling nearly 49% of all of the datapoints. This was a successful result as this method allowed for better prediction than random assignment, which would have yielded about 25% accuracy, given that there are only four target values, each corresponding to a mood: sad, happy, energetic, and calm. GMM follows closely behind KMeans, allowing for about 40% accuracy, despite creating better clusters when scored across the Adjusted Rand, Jaccard, and Recall scores. While DBScan still performed slightly better than random assignment of the target values, it predicted the moods with the least accuracy at just 28%.</p>
    <h3>Supervised Learning Results and Discussion</h3>
    <p>For supervised learning, two methods were chosen: neural networks and random forest. The neural network was chosen due to its flexibility and effectiveness in solving both linear and non-linear problems. Additionally, based on literature review, neural networks were commonly chosen for multi-classification problems. Random forest was chosen to compare the effectiveness of ensemble learning to just one model. Given how well ensemble learning works for predictions, and evidence that it improves accuracy and performance, the random forest approach was pursued. These two methods encompass our supervised learning approach described below.</p>
    <p>The data was first rebalanced using SMOTE, which allowed us to oversample the minority classes to match the number of happy songs. After rebalancing, the data was split into 80% used for training and 20% used for testing.</p>
    <img src="Final Report Pictures/Final-Figure21.png" alt="Figure 21" class="center">
    <figcaption>Figure 21: Rebalanced data set so that all classes have an equal 106,000 songs.</figcaption>
    <h4>Neural Network</h4>
    <p>The neural network used for this problem consisted of an input layer with 7 dimensions (using the reduced feature space before dimensionality reduction), a hidden layer with 10 neurons, and an output layer of 4 dimensions. The Adam optimizer was utilized, giving a constant learning rate of 0.001. After 12 epochs, the model leveled out at a test accuracy of 87.4% and a test loss of 0.3075 (Figure 20). A matrix of the classification results is shown in Figure 23, while a summary of the precision and recall for each class from Figure 22 is shown in Table 1.</p>
    <img src="Final Report Pictures/Final-Figure22.png" alt="Figure 22" class="center">
    <figcaption>Figure 22: The accuracy and loss plots for the neural network over 12 epochs. The test accuracy at the end was 87.4%, while the test loss was 0.3075.</figcaption>
    <img src="Final Report Pictures/Final-Figure23.png" alt="Figure 23" class="center">
    <figcaption>Figure 23: Matrix showing the results of our neural network on the validation set. Columns indicate the predicted classes, while rows indicate the ground truths; elements on the main diagonal are the correctly classified songs.</figcaption>
    <figcaption>Table 1: Precision and Recall Metrics for each class.</figcaption>
    <img src="Final Report Pictures/Final-Table1.png" alt="Table 1" class="center">
    <p>Overall, the Neural network performed best at classifying calm songs, scoring over 90% with both precision and recall. The model performed the worst with the happy songs, scoring just barely over 80% with both metrics. Still, the overall accuracy of the model was 87%, far exceeding the minimum threshold of 25% classification accuracy. For a song recommendation system, higher precision would be more important since users would want the most relevant recommendations ; for the neural network, both metrics ended up returning comparable results. The Neural Network is known for being robust and a great general purpose supervised learning model, so its success for our linear classification problem is hardly surprising . Prior literature and experiments have shown strong neural network performance on similar multiclass classification problems, and our results are consistent with that [7].</p>
    <h4>Random Forest</h4>
    <p>The Random Forest classifier utilized for this project was optimized to have 60 trees. This was found by running the classifier for various numbers of trees and determining a tree based on a cutoff value for accuracy (Figure 24). A matrix of the classification results is shown in Figure 25, while a summary of the precision and recall for each class is shown in Table 2.</p>
    <img src="Final Report Pictures/Final-Figure24.png" alt="Figure 24" class="center">
    <figcaption>Figure 24: Test Accuracy of Random Forest Classifier Given Number of Trees</figcaption>
    <img src="Final Report Pictures/Final-Figure25.png" alt="Figure 25" class="center">
    <figcaption>Figure 25: Matrix showing the results of Random Forest on the validation set. Columns indicate the predicted classes, while rows indicate the ground truths; elements on the main diagonal are the correctly classified songs.</figcaption>
    <p>The model yielded an overall precision and recall value of approximately 96%. The model performed best on calm songs with precision and recall values above 98% and worst on happy songs with precision and recall values around 93%. This model clearly exceeds the minimum threshold of success set previously in the project which was a model with 25% classification accuracy. This was derived from the expected value of the classification accuracy if the songs were randomly sorted into categories. As stated previously, precision is much more important for a song recommendation system given that the users require relevant recommendations. While this is the case, the precision and recall values were similar enough that both could be used to evaluate the model and yield the same conclusions on the model ability. Random Forest was expected to work well with the set of data provided given that it works well with classification data and its ability to take on large datasets. Literature on specifically music classifiers shows that the Random Forest classifier is expected to perform the best out of the available models [8].</p>
    <figcaption>Table 2: Precision and Recall Metrics for each class for Random Forest.</figcaption>
    <img src="Final Report Pictures/Final-Table2.png" alt="Table 2" class="center">
    <h4>Supervised Learning Methods Comparison</h4>
    <p>When the Neural Network and Random Forest techniques are compared with each other, it is clear that the Random Forest ensemble learning algorithm worked the best for classifying the moods of songs for each of the four categories. The precision metric was the focus of comparison, given that for a song recommender we expect relevant recommendations consistently.</p>
    <figcaption>Table 3: Precision and Recall Metrics for each class Summary</figcaption>
    <img src="Final Report Pictures/Final-Table3.png" alt="Table 3" class="center">
    <p>Random Forest had an average accuracy of 96% for both the precision and recall measures. This was about 9.45% better than the precision and recall measures from the Neural Network technique. The difference was most clear in classifying Happy songs, with a 15.64% difference in precision, and a 13.55% difference in recall. The Neural Network was most accurate in identifying Calm songs, but the accuracy measures were still not higher than the Sad or Calm categories classified by the Random Forest, and similar to the accuracy measures of Happy and Energetic. Both the Random Forest and the Neural Network correctly classified the Calm Songs at the highest rate than all other categories, and yet was the most similar in accuracy measures between the two techniques at a difference of 4.45% in precision and 2.90% in recall. For each song mood, Random Forest was the best supervised learning technique for classifying the songs in the dataset based on their features.</p>
    <h2>Conclusions and Takeaways</h2>
    <p>This project was very successful as both unsupervised and supervised techniques provided more accurate song mood classification than random assignment, which would likely yield a 25% classification accuracy. While the K-Means algorithm allowed 50% classification accuracy, the Random Forest algorithm delivered an impressive 96% classification accuracy. This number was much higher than expected when the research commenced. As expected, the ensemble learning algorithm performed much better than a single model, as the Random Forest algorithm utilizes many decision trees for classification. The success of this project may be attributed to the large size of the dataset, which allowed for comprehensive training data. In addition, the set included 11 features, which is quite considerable when compared to the measurable features of other subjects.</p>
    <p>There is still more work that can be completed to improve the results found in this study. One option is automating the search for proper Neural Network parameters. This would allow for easier fine-tuning of the algorithm and may have led to better Neural network results. Because Random Forest outperformed Neural Networks significantly, this avenue was set aside for future work. The Neural Network might also be improved by utilizing K-Folds Validation to get an average Neural Network accuracy. In the project proposal, creating an ensemble of both Decision Trees and Neural Networks was anticipated to be necessary in order to lead to the best results. However, the Neural Network technique did not perform better than the Random Forest ensemble in any mood category, so the Decision Tree and Neural Network combination was not attempted. If the Neural Network accuracy for any mood category was improved with any of the aforementioned future work considerations, then an ensemble of Neural Network and Decision Tree may yield even better results than already achieved.</p>
    <h2>References</h2>
        <citation class="container">
          [1] A. Orr, “Streaming accounted for 84% of the music industry's revenue in 2022,” AppleInsider,<a href="https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time">https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time</a> (accessed Jun. 16, 2023).
        </citation>
        <p>
            <div class="container">
                [1] A. Orr, “Streaming accounted for 84% of the music industry's revenue in 2022,” AppleInsider,<a href="https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time">https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time</a> (accessed Jun. 16, 2023).
            </div><br>
            <citations>
                [2] S. Heshmat, “Music, emotion, and well-being,” Psychology Today,<br>
                <a href=https://www.psychologytoday.com/us/blog/science-choice/201908/music-emotion-and-well-being>https://www.psychologytoday.com/us/blog/science-choice/201908/music-emotion-and-well-being</a> (accessed Jun. 16, 2023).
            </citations><br>
            <citations>[3] N. Rathee Singh and N. Goyal. (2018). Musical preferences and their influence on emotional states and the orientation towards life.</citations><br>
            <citations>[4] A. S. Bhat, V. S. Amith, N. S. Prasad and D. M. Mohan, "An Efficient Classification Algorithm for Music Mood Detection in Western and Hindi Music Using Audio Feature Extraction," 2014 Fifth International Conference on Signal and Image Processing, Bangalore, India, 2014, pp. 359-364, doi: 10.1109/ICSIP.2014.63.</citations><br>
            <citations>[5] A. Orzan, A. Rzayev, G. Altinisik, and Y. Dinçer. (2023, June). 278k Emotion Labeled Spotify Songs, Version 2. Retrieved June 16, 2023 from <a href="https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset">https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset</a></citations><br>
            <citations>[6] “Dealing with Highly Dimensional Data using Principal Component Analysis (PCA) | by Isabella Lindgren | Towards Data Science.” <a href="https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6">https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6</a> (accessed Jul. 07, 2023).</citations><br>
            <citations>[7] C. Veas, “Predicting the Music Mood of a Song with Deep Learning.,” Medium, Sep. 13, 2020. <a href="https://towardsdatascience.com/predicting-the-music-mood-of-a-song-with-deep-learning-c3ac2b45229e">https://towardsdatascience.com/predicting-the-music-mood-of-a-song-with-deep-learning-c3ac2b45229e</a> (accessed Jul. 23, 2023).</citations><br>
            <citations>[8] J. Kamal, P. Priya, M. R. Anala and G. R. Smitha, "A Classification Based Approach to the Prediction of Song Popularity," 2021 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES), Chennai, India, 2021, pp. 1-5, doi: 10.1109/ICSES52305.2021.9633884.</citations><br>
        </p>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script>/* Toggle between adding and removing the "responsive" class to topnav when the user clicks on the icon */
        function myFunction() {
          var x = document.getElementById("topNavElem");
          if (x.className === "topnav") {
            x.className += " responsive";
          } else {
            x.className = "topnav";
          }
        }</script>
</body>
</html>