<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Midterm Report</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <div class="topnav" id = "topNavElem">
        <a href="https://shayan-boghani.github.io/cs4641project">CS 4641 Group 3 Project</a>
        <div class="topnav-right">
            <a href="https://shayan-boghani.github.io/cs4641project/proposal">Proposal</a>
            <a class = "active" href="https://shayan-boghani.github.io/cs4641project/midterm-report">Midterm Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/final-report">Final Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/references">References</a>
            <a href="javascript:void(0);" class="icon" onclick="myFunction()">
                <i class="fa fa-bars"></i>
            </a>
        </div>
      </div>
      <h1>Midterm Report</h1>
    <img src="new_infographic.jpg" alt="Infographic" class="center">
    <h2>Introduction</h2>
        <p>The music streaming industry is one of the largest sectors of the entertainment industry, representing a 10.2 billion dollar revenue stream in 2022 alone [1]. This industry is also highly competitive, and players are consistently looking at methods to optimize the listening experience for their users. One of the primary methods in which this is accomplished is through song recommendations based on listening patterns. Currently, this is primarily done using concrete attributes of music, such as tempo, lyrics, and artists to name a few. While this is an adequate first step, there are methods in which this can be improved. Emotion and music are significantly interlinked, with emotions affecting music choice and vice versa [2]. Singh & Goyal (2018) discuss the effect music has on emotion, and this is the motivation behind focusing on this new classification model [3]. Our team aims to improve the song recommendation concept by developing a tool that can classify songs on Spotify based on the primary emotion presented by the song. The primary emotions will be classified using Thayer’s mood model [4]. By developing this additional classification, our group sees potential in the application of this technology to further improve music recommendation technology and provide a better overall user experience.</p>
    <h2>Methodology</h2>
        <p>The dataset being used is the “278k Emotion Labeled Spotify Songs” found on Kaggle [6]. The dataset contains 11 features describing 278,000 songs. The features include the metrics Acousticness, Danceability, Energy, Instrumentalness, Liveliness, Speechiness, Valence, Loudness, and Tempo. All features are continuous numeric values, and all except Loudness, Tempo, Duration, and Spec_Rate features are limited to a range of 0 to 1. The target values for each song are discrete integers from 0 to 3, with each integer representing a different mood. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm. </p>
        <p>The first steps involve preprocessing and standardizing the dataset. Most of the features are already normalized, so the four non-normalized features (Loudness, Tempo, Duration, and Spec_Rate) will be normalized to the same [0, 1] scale. Since all features are already represented numerically, there is no need for additional feature engineering. Next, we will be looking at the variances, covariances, and normalized mutual information with the target values to better understand the relationship between the features and to determine which features can be dropped. With the remaining features selected, we plan to utilize Linear PCA to reduce our reduced feature space to a smaller number of principle components that can represent at least 80% of the variance in the data [5].</p>
        <p>The methods utilized for unsupervised learning were K-Means, GMM, and DBScan each utilized for clustering. These methods will focus on identifying patterns and groups based on the reduced feature space from our dimensionality reduction. The performance of each clustering method will be evaluated with the Folkes-Mallows measure, Normalized Mutual Information, Adjusted Rand statistic, Davies-Bouldin index, Jaccard coefficient, and Recall score.</p>
    <h2>Results and Discussion</h2>
    <h3><center>Feature Selection and Dimensionality Reduction</center></h3>
        <p>After all features were normalized to the same [0, 1] scale, statistical feature selection was conducted. First, the variances and covariances of all features were calculated and are shown in the figures below.</p>
        <img src="new_infographic.jpg" alt="Infographic" class="center">
        <figcaption><center>Figure 1: Variances of all features in the dataset.</center></figcaption>
        <img src="new_infographic.jpg" alt="Infographic" class="center">
        <figcaption><center>Figure 2: Covariance Matrix for all features in the dataset.</center></figcaption>
        <p>The variance chart in Figure 1 shows that some features have high variances, indicating features such as Instrumentalness and acousticness could be helpful to classify songs. Both duration and sample rate of the songs had incredibly low variance values, so those two features were dropped. Logically, it also follows that the sample rate and length of the song will likely not affect the mood of the song. The covariance matrix in Figure 2 shows the relationships between each feature, and most have no strong correlation with the other variables. There is one relatively strong (negative) correlation value between the energy and acousticness of the song, with a covariance of -0.079. Between these two features, one of them can also be dropped. We determined which of the two features to drop by then calculating the mutual information between all features and the target values.</p>
        <img src="new_infographic.jpg" alt="Infographic" class="center">
        <figcaption><center>Figure 3: Normalized mutual information heatmap of all original eleven features.</center></figcaption>
        <img src="new_infographic.jpg" alt="Infographic" class="center">
        <figcaption><center>Figure 4: Mutual information graph between all original eleven features and the target.</center></figcaption>
        <p>The mutual information between all features is shown in Figure 3, and it shows very small NMI values between all the features. This means that there is very little co-dependence between the features. This alone does not improve the efforts to reduce dimensionality; however, finding the mutual information between each feature and the target variables will help eliminate some features. The chart in Figure 4 shows these shared mutual information values. From the chart, acousticness has a slightly lower mutual information with the target than energy, so the acousticness feature was also dropped. The liveness and tempo features were also ultimately dropped; of all remaining features, those two had the lowest mutual information with the target.</p>
        <p>Looking at these statistics allowed for the feature space to be simplified to 6 components: Loudness, Energy, Instrumentalness, Danceability, Valence, and Speechiness. From here, Linear PCA was selected to reduce the complexity of our problem further; the initial dataset distribution also implied a linear separation of data, making Linear PCA a good fit. The results of the PCA can be seen in Figure 5.</p>
        <img src="new_infographic.jpg" alt="Infographic" class="center">
        <figcaption><center>Figure 5: Scree plot for the Spotify Song Dataset</center></figcaption>
        <p>The goal was to reduce the problem to a number of principle components until the total recovered variance is at least above 80%. With the PCA on the simplified feature space, we found that the use of 3 principal components allowed us to represent the feature space with 90% of the variance recovered, exceeding our thresholds. As a result of all the feature selection and dimensionality reduction steps, the problem was reduced from an 11-dimensional feature space to a 3-dimensional feature space with three principal components.</p>
    <h2>References</h2>
        <p>
            <citations>
                [1] A. Orr, “Streaming accounted for 84% of the music industry's revenue in 2022,” AppleInsider,<br>
                https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time (accessed Jun. 16, 2023).
            </citations><br>
            <citations>
                [2] S. Heshmat, “Music, emotion, and well-being,” Psychology Today,<br>
                https://www.psychologytoday.com/us/blog/science-choice/201908/music-emotion-and-well-being (accessed Jun. 16, 2023).
            </citations><br>
            <citations>[3] A. S. Bhat, V. S. Amith, N. S. Prasad and D. M. Mohan, "An Efficient Classification Algorithm for Music Mood Detection in Western and Hindi Music Using Audio Feature Extraction," 2014 Fifth International Conference on Signal and Image Processing, Bangalore, India, 2014, pp. 359-364, doi: 10.1109/ICSIP.2014.63.</citations><br>
            <citations>[4] N. Rathee Singh and N. Goyal. (2018). Musical preferences and their influence on emotional states and the orientation towards life.</citations><br>
            <citations>[5] A. Orzan, A. Rzayev, G. Altinisik, and Y. Dinçer. (2023, June). 278k Emotion Labeled Spotify Songs, Version 2. Retrieved June 16, 2023 from https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset.</citations><br>
            <citations>[6] Y. Seo and J. Huh. (2019). Automatic Emotion-Based Music Classification for Supporting Intelligent IoT Applications. Electronics. 8. 164. 10.3390/electronics8020164.</citations><br>
            <citations>[7] G. Baltazar, “CPU vs GPU in Machine Learning,” Oracle Blogs, https://blogs.oracle.com/ai-and-datascience/post/cpu-vs-gpu-in-machine-learning (accessed Jun. 16, 2023). </citations><br>
        </p>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script>/* Toggle between adding and removing the "responsive" class to topnav when the user clicks on the icon */
        function myFunction() {
          var x = document.getElementById("topNavElem");
          if (x.className === "topnav") {
            x.className += " responsive";
          } else {
            x.className = "topnav";
          }
        }</script>
</body>
</html>
