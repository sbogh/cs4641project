<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Midterm Report</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <div class="topnav" id = "topNavElem">
        <a href="https://shayan-boghani.github.io/cs4641project">CS 4641 Group 3 Project</a>
        <div class="topnav-right">
            <a href="https://shayan-boghani.github.io/cs4641project/proposal">Proposal</a>
            <a class = "active" href="https://shayan-boghani.github.io/cs4641project/midterm-report">Midterm Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/final-report">Final Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/references">References</a>
            <a href="javascript:void(0);" class="icon" onclick="myFunction()">
                <i class="fa fa-bars"></i>
            </a>
        </div>
      </div>
      <h1>Proposal</h1>
    <img src="Updated Infographic.png" alt="Infographic" class="center">
    <h2>Introduction</h2>
        <p>The music streaming industry is one of the largest sectors of the entertainment industry, representing a 10.2 billion dollar revenue stream in 2022 alone [1]. This industry is also highly competitive, and players are consistently looking at methods to optimize the listening experience for their users. One of the primary methods in which this is accomplished is through song recommendations based on listening patterns. Currently, this is primarily done using concrete attributes of music, such as tempo, lyrics, and artists to name a few. While this is an adequate first step, there are methods in which this can be improved. Emotion and music are significantly interlinked, with emotions affecting music choice and vice versa [2]. Singh & Goyal (2018) discuss the effect music has on emotion, and this is the motivation behind focusing on this new classification model [3]. Our team aims to improve the song recommendation concept by developing a tool that can classify songs on Spotify based on the primary emotion presented by the song. The primary emotions will be classified using Thayer’s mood model [4]. By developing this additional classification, our group sees potential in the application of this technology to further improve music recommendation technology and provide a better overall user experience.</p>
    <h2>Methodology</h2>
        <p>There exist many similar music classification models already. Seo & Huh (2019) utilize surveys to determine the primary emotion of the music used, whereas the dataset we are utilizing works with Thayer’s mood model to classify the songs into the four different emotion categories [5]. This allows for a broader classification of songs, allowing for more unique suggestions.</p>
        <p>The dataset being used is the “278k Emotion Labeled Spotify Songs” found on Kaggle [6]. The dataset contains 11 features describing 278,000 songs. The features include the metrics Acousticness, Danceability, Energy, Instrumentalness, Liveliness, Speechiness, Valence, Loudness, and Tempo. All features are continuous numeric values, and all except the Loudness and Tempo features are limited to a range of 0 to 1. To match the other features given, we will preprocess the Loudness and Tempo values and normalize them to fit within a similar 0 to 1 range. The target values for each song are discrete integers from 0 to 3, with each integer representing a different mood. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm.</p>
        <p>The dataset's features are represented numerically, so they can be used directly for modeling without extensive feature engineering. If deemed necessary, additional features can be derived from the existing ones to further infer the emotion of the music, though it will likely not be needed. Because there are eleven features in the dataset, dimensionality reduction techniques will be necessary to reduce the complexity of the problem. One dimensionality approach that will be considered is Principal Component Analysis (PCA), which uses the covariance of the matrix of normalized data and eigen decomposition to identify the principal components that capture the most significant variations in the data.</p>
        <p>This project will use unsupervised learning to cluster and compare the results with the target values. K-means clustering, hierarchical clustering, and density-based clustering techniques will allow for the identification of patterns and groups based on the song features. Supervised learning will be utilized to classify the songs correctly. Decision trees, random forests, and neural networks will be useful by training them on the target dataset. We predict a neural network will be most successful in classifying the songs, though further research is needed. The main risk in this project is the time and resources needed. Though our dataset is large, the features are all numeric, so time should not be a major concern. The size of the dataset may require use of a GPU to speed up the training process, as our model will be performing many matrix calculations simultaneously [7]. We plan to utilize the GPU resources on Google Colab as necessary.</p>
        <p>The timeline for this project is approximately a month and a half, with two checkpoints to ensure steady progress is made. The first half of the workflow will focus on the unsupervised learning aspect and understanding which clustering techniques are best suited for the data set. The second half will focus on supervised learning and classification of the songs. The goal is to have a model that meets the requirements listed below by the end of the project.</p>
    <h2>Results</h2>
        <p>The expected results of this project are to be able to classify individual songs based on their primary emotion. The metric for success would be at minimum 25% sorting accuracy. This is the value that the system would theoretically get right by chance, so the model should outperform this metric. The metrics for success would focus on precision. This is because we are more focused on minimizing mistakes in the outputs rather than completely representing the ground truths. A possible metric for Unsupervised Learning would be the Silhouette Coefficient to understand the goodness of the clusters. Another metric would be the Rand Statistic for Supervised Learning to determine the fraction of accurately sorted songs. The mid-term “exam” for this system will be the Unsupervised Learning portion of the project. The final “exam” will be the Supervised Learning portion of the project. In both exams we will determine, based on the metric of success, whether the tool is capable of sorting the songs properly.</p>
    <h2>Discussion</h2>
        <p>With the ever-growing set of music available to the public, it becomes overwhelming at times to find new music, and many times music may be lost in the sea of new content available. Using this tool, consumers would be provided with music which matches the primary emotion in the songs they listen to and allows for a more positive response to recommendations. Additionally, the system could sort and build playlists based on a core emotion for users, again allowing consumers to reach music which they may not be able to find from a simple search. This model could work in tandem with existing methods to provide new suggestions not limited by genre or artist alone. Smaller artists would benefit from a more generalized suggestion model, as grouping by mood could result in their music reaching new audiences.</p>
    <h2>References</h2>
        <p>
            <citations>
                [1] A. Orr, “Streaming accounted for 84% of the music industry's revenue in 2022,” AppleInsider,<br>
                https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time (accessed Jun. 16, 2023).
            </citations><br>
            <citations>
                [2] S. Heshmat, “Music, emotion, and well-being,” Psychology Today,<br>
                https://www.psychologytoday.com/us/blog/science-choice/201908/music-emotion-and-well-being (accessed Jun. 16, 2023).
            </citations><br>
            <citations>[3] A. S. Bhat, V. S. Amith, N. S. Prasad and D. M. Mohan, "An Efficient Classification Algorithm for Music Mood Detection in Western and Hindi Music Using Audio Feature Extraction," 2014 Fifth International Conference on Signal and Image Processing, Bangalore, India, 2014, pp. 359-364, doi: 10.1109/ICSIP.2014.63.</citations><br>
            <citations>[4] N. Rathee Singh and N. Goyal. (2018). Musical preferences and their influence on emotional states and the orientation towards life.</citations><br>
            <citations>[5] A. Orzan, A. Rzayev, G. Altinisik, and Y. Dinçer. (2023, June). 278k Emotion Labeled Spotify Songs, Version 2. Retrieved June 16, 2023 from https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset.</citations><br>
            <citations>[6] Y. Seo and J. Huh. (2019). Automatic Emotion-Based Music Classification for Supporting Intelligent IoT Applications. Electronics. 8. 164. 10.3390/electronics8020164.</citations><br>
            <citations>[7] G. Baltazar, “CPU vs GPU in Machine Learning,” Oracle Blogs, https://blogs.oracle.com/ai-and-datascience/post/cpu-vs-gpu-in-machine-learning (accessed Jun. 16, 2023). </citations><br>
        </p>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script>/* Toggle between adding and removing the "responsive" class to topnav when the user clicks on the icon */
        function myFunction() {
          var x = document.getElementById("topNavElem");
          if (x.className === "topnav") {
            x.className += " responsive";
          } else {
            x.className = "topnav";
          }
        }</script>
</body>
</html>