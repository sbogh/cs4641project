<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Midterm Report</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <div class="topnav" id = "topNavElem">
        <a href="https://shayan-boghani.github.io/cs4641project">CS 4641 Group 3 Project</a>
        <div class="topnav-right">
            <a href="https://shayan-boghani.github.io/cs4641project/proposal">Proposal</a>
            <a class = "active" href="https://shayan-boghani.github.io/cs4641project/midterm-report">Midterm Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/final-report">Final Report</a>
            <a href="https://shayan-boghani.github.io/cs4641project/references">References</a>
            <a href="javascript:void(0);" class="icon" onclick="myFunction()">
                <i class="fa fa-bars"></i>
            </a>
        </div>
      </div>
      <h1>Midterm Report</h1>
    <img src="new_infographic.jpg" alt="Infographic" class="center">
    <h2>Introduction</h2>
        <p>The music streaming industry is one of the largest sectors of the entertainment industry, representing a 10.2 billion dollar revenue stream in 2022 alone [1]. This industry is also highly competitive, and players are consistently looking at methods to optimize the listening experience for their users. One of the primary methods in which this is accomplished is through song recommendations based on listening patterns. Currently, this is primarily done using concrete attributes of music, such as tempo, lyrics, and artists to name a few. While this is an adequate first step, there are methods in which this can be improved. Emotion and music are significantly interlinked, with emotions affecting music choice and vice versa [2]. Singh & Goyal (2018) discuss the effect music has on emotion, and this is the motivation behind focusing on this new classification model [3]. Our team aims to improve the song recommendation concept by developing a tool that can classify songs on Spotify based on the primary emotion presented by the song. The primary emotions will be classified using Thayer’s mood model [4]. By developing this additional classification, our group sees potential in the application of this technology to further improve music recommendation technology and provide a better overall user experience.</p>
    <h2>Methodology</h2>
        <p>The dataset being used is the “278k Emotion Labeled Spotify Songs” found on Kaggle [6]. The dataset contains 11 features describing 278,000 songs. The features include the metrics Acousticness, Danceability, Energy, Instrumentalness, Liveliness, Speechiness, Valence, Loudness, and Tempo. All features are continuous numeric values, and all except Loudness, Tempo, Duration, and Spec_Rate features are limited to a range of 0 to 1. The target values for each song are discrete integers from 0 to 3, with each integer representing a different mood. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm. </p>
        <p>The first steps involve preprocessing and standardizing the dataset. Most of the features are already normalized, so the four non-normalized features (Loudness, Tempo, Duration, and Spec_Rate) will be normalized to the same [0, 1] scale. Since all features are already represented numerically, there is no need for additional feature engineering. Next, we will be looking at the variances, covariances, and normalized mutual information with the target values to better understand the relationship between the features and to determine which features can be dropped. With the remaining features selected, we plan to utilize Linear PCA to reduce our reduced feature space to a smaller number of principle components that can represent at least 80% of the variance in the data [5].</p>
        <p>The methods utilized for unsupervised learning were K-Means, GMM, and DBScan each utilized for clustering. These methods will focus on identifying patterns and groups based on the reduced feature space from our dimensionality reduction. The performance of each clustering method will be evaluated with the Folkes-Mallows measure, Normalized Mutual Information, Adjusted Rand statistic, Davies-Bouldin index, Jaccard coefficient, and Recall score.</p>
    <h2>Results and Discussion</h2>
    <h3><center>Feature Selection and Dimensionality Reduction</center></h3>
        <p>After all features were normalized to the same [0, 1] scale, statistical feature selection was conducted. First, the variances and covariances of all features were calculated and are shown in the figures below.</p>
        <img src="fig1.png" alt="Infographic" class="center">
        <figcaption><center>Figure 1: Variances of all features in the dataset.</center></figcaption>
        <img src="fig2.png" alt="Infographic" class="center">
        <figcaption><center>Figure 2: Covariance Matrix for all features in the dataset.</center></figcaption>
        <p>The variance chart in Figure 1 shows that some features have high variances, indicating features such as Instrumentalness and acousticness could be helpful to classify songs. Both duration and sample rate of the songs had incredibly low variance values, so those two features were dropped. Logically, it also follows that the sample rate and length of the song will likely not affect the mood of the song. The covariance matrix in Figure 2 shows the relationships between each feature, and most have no strong correlation with the other variables. There is one relatively strong (negative) correlation value between the energy and acousticness of the song, with a covariance of -0.079. Between these two features, one of them can also be dropped. We determined which of the two features to drop by then calculating the mutual information between all features and the target values.</p>
        <img src="fig3.png" alt="Infographic" class="center">
        <figcaption><center>Figure 3: Normalized mutual information heatmap of all original eleven features.</center></figcaption>
        <img src="fig4.png" alt="Infographic" class="center">
        <figcaption><center>Figure 4: Mutual information graph between all original eleven features and the target.</center></figcaption>
        <p>The mutual information between all features is shown in Figure 3, and it shows very small NMI values between all the features. This means that there is very little co-dependence between the features. This alone does not improve the efforts to reduce dimensionality; however, finding the mutual information between each feature and the target variables will help eliminate some features. The chart in Figure 4 shows these shared mutual information values. From the chart, acousticness has a slightly lower mutual information with the target than energy, so the acousticness feature was also dropped. The liveness and tempo features were also ultimately dropped; of all remaining features, those two had the lowest mutual information with the target.</p>
        <p>Looking at these statistics allowed for the feature space to be simplified to 6 components: Loudness, Energy, Instrumentalness, Danceability, Valence, and Speechiness. From here, Linear PCA was selected to reduce the complexity of our problem further; the initial dataset distribution also implied a linear separation of data, making Linear PCA a good fit. The results of the PCA can be seen in Figure 5.</p>
        <img src="fig5.png" alt="Infographic" class="center">
        <figcaption><center>Figure 5: Scree plot for the Spotify Song Dataset</center></figcaption>
        <p>The goal was to reduce the problem to a number of principle components until the total recovered variance is at least above 80%. With the PCA on the simplified feature space, we found that the use of 3 principal components allowed us to represent the feature space with 90% of the variance recovered, exceeding our thresholds. As a result of all the feature selection and dimensionality reduction steps, the problem was reduced from an 11-dimensional feature space to a 3-dimensional feature space with three principal components.</p>
    <h3><center>Unsupervised Learning Methods</center></h3>
        <img src="fig6.png" alt="Infographic" class="center">
        <figcaption><center>Figure 6: Distribution of songs in the dataset. 0 corresponds to Sad, 1 corresponds to Happy, 2 corresponds to Energetic, and 3 corresponds to Calm.</center></figcaption>
        <p>Before clustering, it is important to first understand the distribution of the four classes in our dataset. Figure 6 shows the frequency of each target class in the dataset, with the number of happy songs (mood 1) being much larger than the other classes, exceeding 100,000 songs out of 278,000 total. Thus, we can expect the cluster of happy songs to contain the most data points. While not a major concern for now, for the supervised learning portions of the project, careful selection of the data is necessary to ensure the training and testing datasets are more closely balanced. This likely means taking fewer of the happy and sad songs to match the frequency of the other two remaining classes, which both have around 40,000 songs.</p>
    <h4><center>K-Means</center></h4>
        <p>After reducing dimensionality, K-Means was used to identify any obvious clusters. With each of the three principal components mapped against each other, as seen in the graphs, there are no obvious clusters. This can be checked with the Fowlkes-Mallows, Rand, and NMI scores. None of the clusterings resulted in a Fowlkes-Mallows score above 0.55 while none had a Rand or NMI score above 0.38. This further justifies the poor clustering with K-Means. Two of the clusterings, however, had a correct classification just below fifty percent. This is improved upon a mere random guess, which would have a correct classification rate of about twenty-five percent.</p>
        <p>After looking at the results, it was clear the K-Means could not effectively cluster the data so GMM and DBSCAN should also be computed to compare the different clustering methods.</p>
        <img src="fig7.png" alt="Infographic" class="center">
        <figcaption><center>Figure 7: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph. </center></figcaption>
        <img src="fig8.png" alt="Infographic" class="center">
        <figcaption><center>Figure 8: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph.</center></figcaption>
        <img src="fig9.png" alt="Infographic" class="center">
        <figcaption><center>Figure 9: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right with red diamonds marking the center of each cluster. Colors are not directly correlated between classes on each graph.</center></figcaption>
    <h4><center>GMM</center></h4>
        <p>Along with K-Means, GMM was utilized for clustering. The GMM model was run for each of the three principal components compared to one another (1-2, 1-3, 2-3). The results showed that GMM was not a strong clustering technique when used with this data set. This is quantified using the Fowlkes-Mallows, Rand, and NMI scores. The maximum Fowlkes-Mallows score recorded was 0.491 and the maximum scores for Rand and NMI were 0.3 and 0.32, respectively. All these values were recorded for the comparison of principal components 1 and 3. The maximum clustering correctness was recorded at approximately sixty-five percent. This value is significantly higher than the randomized outcome which is expected which is approximately twenty-five percent.</p>
        <p>Something to note is that GMM is not suited for categorical features, in this case the groupings of songs based on emotions. Given the assumption of normal distribution of all features, clustering can be quite difficult. As seen from the results, this is the case with this data set. It can be concluded that GMM is not a valid approach for this data set based on the fundamental issues associated with utilizing the method for categorical data. </p>
        <img src="fig10.png" alt="Infographic" class="center">
        <figcaption><center>Figure 10: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</center></figcaption>
        <img src="fig11.png" alt="Infographic" class="center">
        <figcaption><center>Figure 11: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph. </center></figcaption>
        <img src="fig12.png" alt="Infographic" class="center">
        <figcaption><center>Figure 12: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph on the right. Colors are not directly correlated between classes on each graph.</center></figcaption>
    <h4><center>DBScan</center></h4>
        <p>In addition to K-Means and GMM, DBScan was used in clustering. As with both other clustering methods, DBScan was run with 3 combinations of principal components – 1-2, 1-3 and 2-3. To optimize the clustering methodology, MinPts was found to be 4 due to the use of 3 principal components, and eps was found to be 0.003 using the elbow method (Figure 13).</p>
        <img src="fig13.png" alt="Infographic" class="center">
        <figcaption><center>Figure 13: Eps calculation based on 4th nearest neighbor. Eps used: 0.003 </center></figcaption>
        <p>This clustering method was found to be rather poor in representing the clusters of this dataset. This was numerically justified through the use of the Fowlkes-Mallows, Rand, and NMI clustering evaluation scores. The maximum scores recorded for these metrics were 0.512, 0.187, and 0.217 respectively. In addition, the maximum accuracy using this method was only around 27%, just slightly higher than random selection which is 25%. The clusters of each combination of principal components can be visualized in Figures 13-15 below.</p>
        <p>Multiple reasons for this inaccuracy are possible. One of the major limitations of DBscan is that it is not optimal at clustering datasets with varying densities, which is a possibility in this case. In addition, DBscan is highly dependent on the value of the eps parameter. While visually optimized using the elbow method and the above graph, it is possible that the true optimal parameter was not found. </p>
        <img src="fig14.png" alt="Infographic" class="center">
        <figcaption><center>Figure 14: Graphs representing Principal Component 1 (X-axis) and Principal Component 2 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</center></figcaption>
        <img src="fig15.png" alt="Infographic" class="center">
        <figcaption><center>Figure 15: Graphs representing Principal Component 1 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</center></figcaption>
        <img src="fig16.png" alt="Infographic" class="center">
        <figcaption><center>Figure 16: Graphs representing Principal Component 2 (X-axis) and Principal Component 3 (Y-axis). Original clusters graph is on the left. Predicted clusters graph using DBScan on the right. Colors are not directly correlated between classes on each graph.</center></figcaption>
    <h4><center>Unsupervised Learning Methods Comparison</center></h4>
        <img src="fig17.png" alt="Infographic" class="center">
        <figcaption><center>Figure 17: Graphs comparing the scores for Fowlkes-Mallows, NMI and Adjusted Rand of each clustering technique: KMeans, DBScan, and GMM respectively.</center></figcaption>
        <img src="fig18.png" alt="Infographic" class="center">
        <figcaption><center>Figure 18: Graphs comparing the scores for Davies-Bouldin, Jaccard and Recall of each clustering technique: KMeans, DBScan, and GMM respectively.</center></figcaption>
        <p>The clustering techniques performed most similarly when compared using the Fowlkes Mallows score. However, though DBScan performed better than both GMM and KMeans when scored by Fowlkes Mallows, it yielded the least accuracy of proper classification. The better scoring measures for this application are NMI and Adjusted Rand. These more accurately reflect the effectiveness of the predictiveness of the clustering techniques in relation to each other. Of the three clustering techniques, KMeans had the best accuracy at nearly 49%, with GMM trailings at about 40%. This relationship is best reflected by the NMI score. While not the best indicator for this application, the Davies Bouldin, Jaccard, and Recall scores were also calculated for each technique for comparison. GMM scored highly against the Jaccard score and Recall score, indicating that this technique led to the best clusters, yet it did not provide the best prediction results.</p>
        <p>Overall, KMeans clustering was the best unsupervised learning clustering technique, correctly labeling nearly 49% of all of the datapoints. This was a successful result as this method allowed for better prediction than random assignment, which would have yielded about 25% accuracy, given that there are only four target values, each corresponding to a mood: sad, happy, energetic, and calm. GMM follows closely behind KMeans, allowing for about 40% accuracy, despite creating better clusters when scored across the Adjusted Rand, Jaccard, and Recall scores. While DBScan still performed slightly better than random assignment of the target values, it predicted the moods with the least accuracy at just 28%. </p>
    <h2>References</h2>
        <p>
            <citations>
                [1] A. Orr, “Streaming accounted for 84% of the music industry's revenue in 2022,” AppleInsider,<br>
                https://appleinsider.com/articles/23/03/09/streaming-accounted-for-84-of-the-music-industrys-revenue-in-2022#:~:text=The%20Recording%20Industry%20Association%20of,mark%20for%20the%20first%20time (accessed Jun. 16, 2023).
            </citations><br>
            <citations>
                [2] S. Heshmat, “Music, emotion, and well-being,” Psychology Today,<br>
                https://www.psychologytoday.com/us/blog/science-choice/201908/music-emotion-and-well-being (accessed Jun. 16, 2023).
            </citations><br>
            <citations>[3] N. Rathee Singh and N. Goyal. (2018). Musical preferences and their influence on emotional states and the orientation towards life.</citations><br>
            <citations>[4] A. S. Bhat, V. S. Amith, N. S. Prasad and D. M. Mohan, "An Efficient Classification Algorithm for Music Mood Detection in Western and Hindi Music Using Audio Feature Extraction," 2014 Fifth International Conference on Signal and Image Processing, Bangalore, India, 2014, pp. 359-364, doi: 10.1109/ICSIP.2014.63.</citations><br>
            <citations>[5] A. Orzan, A. Rzayev, G. Altinisik, and Y. Dinçer. (2023, June). 278k Emotion Labeled Spotify Songs, Version 2. Retrieved June 16, 2023 from https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset.</citations><br>
            <citations>[6] “Dealing with Highly Dimensional Data using Principal Component Analysis (PCA) | by Isabella Lindgren | Towards Data Science.” https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6 (accessed Jul. 07, 2023).</citations><br>
        </p>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script>/* Toggle between adding and removing the "responsive" class to topnav when the user clicks on the icon */
        function myFunction() {
          var x = document.getElementById("topNavElem");
          if (x.className === "topnav") {
            x.className += " responsive";
          } else {
            x.className = "topnav";
          }
        }</script>
</body>
</html>
